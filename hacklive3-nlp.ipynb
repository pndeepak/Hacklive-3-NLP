{"cells":[{"metadata":{},"cell_type":"markdown","source":"# HackLive 3: Guided Hackathon - NLP (Analytics vidhya)\n\n##### Click <a href='https://datahack.analyticsvidhya.com/contest/hacklive-3-guided-hackathon-text-classification/'>here</a> to go to competition page\n\n#### Author: Palakodeti Nagendra Deepak\n* Performance metric used : Micro F1 score\n* Public leaderboard score: 0.7745629898 \n* Private leaderboard ranking: 7\n* Private leaderboard score: 0.7775161860\n* Private leaderboard ranking: 6"},{"metadata":{},"cell_type":"markdown","source":"<h2> Problem statement </h2>\n\n<h4> In real world scenario many research institutes go through huge archives of research papers, in such scenario tagging of research papers manually becomes a tedious task. The objective of this ML problem is to automatically tag the research paper in any of the 25 possible tags.  </h4>\n    \n**List of possible tags are as follows:**\n\n[Tags, Analysis of PDEs, Applications, Artificial Intelligence,Astrophysics of Galaxies, Computation and Language, Computer Vision and Pattern Recognition, Cosmology and Nongalactic Astrophysics, Data Structures and Algorithms, Differential Geometry, Earth and Planetary Astrophysics, Fluid Dynamics,Information Theory, Instrumentation and Methods for Astrophysics, Machine Learning, Materials Science, Methodology, Number Theory, Optimization and Control, Representation Theory, Robotics, Social and Information Networks, Statistics Theory, Strongly Correlated Electrons, Superconductivity, Systems and Control]\n\n<h2> Type of Machine Learning Problem </h2>\n\n<h4> Since there are 25 different labels and each research paper may belong to one or more category this is a multilabel classification problem </h4>\n\n<h2> Performance metric </h2>\n\n<h4> Micro F1 score </h4>"},{"metadata":{},"cell_type":"markdown","source":"## Importing the data and necessary libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import hstack\nfrom sklearn.metrics import f1_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/analyticsvidhya-hacklive3/Train/Train.csv')\ndf.drop('id', axis=1, inplace=True)\ndf.head(2)","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"                                            ABSTRACT  Computer Science  \\\n0  a ever-growing datasets inside observational a...                 0   \n1  we propose the framework considering optimal $...                 1   \n\n   Mathematics  Physics  Statistics  Analysis of PDEs  Applications  \\\n0            0        1           0                 0             0   \n1            0        0           0                 0             0   \n\n   Artificial Intelligence  Astrophysics of Galaxies  \\\n0                        0                         0   \n1                        0                         0   \n\n   Computation and Language  ...  Methodology  Number Theory  \\\n0                         0  ...            0              0   \n1                         0  ...            0              0   \n\n   Optimization and Control  Representation Theory  Robotics  \\\n0                         0                      0         0   \n1                         0                      0         0   \n\n   Social and Information Networks  Statistics Theory  \\\n0                                0                  0   \n1                                0                  0   \n\n   Strongly Correlated Electrons  Superconductivity  Systems and Control  \n0                              0                  0                    0  \n1                              0                  0                    0  \n\n[2 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ABSTRACT</th>\n      <th>Computer Science</th>\n      <th>Mathematics</th>\n      <th>Physics</th>\n      <th>Statistics</th>\n      <th>Analysis of PDEs</th>\n      <th>Applications</th>\n      <th>Artificial Intelligence</th>\n      <th>Astrophysics of Galaxies</th>\n      <th>Computation and Language</th>\n      <th>...</th>\n      <th>Methodology</th>\n      <th>Number Theory</th>\n      <th>Optimization and Control</th>\n      <th>Representation Theory</th>\n      <th>Robotics</th>\n      <th>Social and Information Networks</th>\n      <th>Statistics Theory</th>\n      <th>Strongly Correlated Electrons</th>\n      <th>Superconductivity</th>\n      <th>Systems and Control</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a ever-growing datasets inside observational a...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>we propose the framework considering optimal $...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 30 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)","execution_count":3,"outputs":[{"output_type":"stream","text":"(14004, 30)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h3> There are total 14004 research papers(rows) in which Abstract gives us the gist of the research paper, rows such as Computer Science, Mathematics, Physics, Statistics gives us the primary domain of the research paper and the remaining 25 columns are the target columns(labels) </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[0, 'ABSTRACT']","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"'a ever-growing datasets inside observational astronomy have challenged scientists inside many aspects, including an efficient and interactive data exploration and visualization. many tools have been developed to confront this challenge. however, they usually focus on displaying a actual images or focus on visualizing patterns within catalogs inside the predefined way. inside this paper we introduce vizic, the python visualization library that builds a connection between images and catalogs through an interactive map of a sky region. vizic visualizes catalog data over the custom background canvas with the help of a shape, size and orientation of each object inside a catalog. a displayed objects inside a map are highly interactive and customizable comparing to those inside a images. these objects should be filtered by or colored by their properties, such as redshift and magnitude. they also should be sub-selected with the help of the lasso-like tool considering further analysis with the help of standard python functions from in the jupyter notebook. furthermore, vizic allows custom overlays to be appended dynamically on top of a sky map. we have initially implemented several overlays, namely, voronoi, delaunay, minimum spanning tree and healpix grid layers, which are helpful considering visualizing large-scale structure. all these overlays should be generated, added or removed interactively with one line of code. a catalog data was stored inside the non-relational database, and a interfaces were developed inside javascript and python to work within jupyter notebook, which allows to create custom widgets, user generated scripts to analyze and plot a data selected/displayed inside a interactive map. this unique design makes vizic the very powerful and flexible interactive analysis tool. vizic should be adopted inside variety of exercises, considering example, data inspection, clustering analysis, galaxy alignment studies, outlier identification or simply large-scale visualizations.'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"<h4> Above is the sample of a Abstract of a research paper </h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/analyticsvidhya-hacklive3/Test/Test.csv')\ntest.head(2)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"      id                                           ABSTRACT  Computer Science  \\\n0   9409  fundamental frequency (f0) approximation from ...                 0   \n1  17934  this large-scale study, consisting of 24.5 mil...                 1   \n\n   Mathematics  Physics  Statistics  \n0            0        0           1  \n1            0        0           1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>ABSTRACT</th>\n      <th>Computer Science</th>\n      <th>Mathematics</th>\n      <th>Physics</th>\n      <th>Statistics</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9409</td>\n      <td>fundamental frequency (f0) approximation from ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17934</td>\n      <td>this large-scale study, consisting of 24.5 mil...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET_COLS = ['Analysis of PDEs', 'Applications',\n               'Artificial Intelligence', 'Astrophysics of Galaxies',\n               'Computation and Language', 'Computer Vision and Pattern Recognition',\n               'Cosmology and Nongalactic Astrophysics',\n               'Data Structures and Algorithms', 'Differential Geometry',\n               'Earth and Planetary Astrophysics', 'Fluid Dynamics',\n               'Information Theory', 'Instrumentation and Methods for Astrophysics',\n               'Machine Learning', 'Materials Science', 'Methodology', 'Number Theory',\n               'Optimization and Control', 'Representation Theory', 'Robotics',\n               'Social and Information Networks', 'Statistics Theory',\n               'Strongly Correlated Electrons', 'Superconductivity',\n               'Systems and Control']","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Text preprocessing </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuations(x):\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’' + '…':\n        x = x.replace(punct, '')\n    return x","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is a generalized replacement of misspelled words which i use for all projects so some words here may not be actually in abstract\ndef misspelled_words(x):\n    x = x.replace('colour', 'color').replace('centre', 'center').replace('didnt', 'did not').replace('doesnt', 'does not') \\\n        .replace('isnt', 'is not').replace('shouldnt', 'should not').replace('favourite', 'favorite').replace('travelling', 'traveling') \\\n        .replace('counselling', 'counseling').replace('theatre', 'theater').replace('cancelled', 'canceled').replace('labour', 'labor') \\\n        .replace('organisation', 'organization').replace('wwii', 'world war 2').replace('citicise', 'criticize') \\\n        .replace('instagram', 'social medium').replace('whatsapp', 'social medium').replace('WeChat', 'social medium') \\\n        .replace('snapchat', 'social medium').replace('Snapchat', 'social medium').replace('btech', 'B.Tech').replace('Quorans', 'Quora') \\\n        .replace('cryptocurrency', 'crypto currency').replace('cryptocurrencies', 'crypto currency').replace('behaviour', 'behavior') \\\n        .replace('analyse', 'analyze').replace('licence', 'license').replace('programme', 'program').replace('grey', 'gray') \\\n        .replace('realise', 'realize').replace('bcom', 'B.Com').replace('defence', 'defense').replace('mtech', 'M.Tech') \\\n        .replace('Btech', 'B.Tech').replace('honours', 'honors').replace('recognise', 'recognize').replace('programr', 'programmer') \\\n        .replace('programrs', 'programmer').replace('hasnt', 'has not').replace('litre', 'liter').replace('Isnt', 'is not') \\\n        .replace('learnt', 'learn').replace('favour', 'favor').replace('neighbour', 'neighbor').replace('demonetisation', 'demonetization') \\\n        .replace('₹', '').replace('&', 'and')\n    return x","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"ABSTRACT\"] = df[\"ABSTRACT\"].apply(lambda x: remove_punctuations(x))\ndf[\"ABSTRACT\"] = df[\"ABSTRACT\"].apply(lambda x: clean_numbers(x))\ndf[\"ABSTRACT\"] = df[\"ABSTRACT\"].apply(lambda x: misspelled_words(x))\ntest[\"ABSTRACT\"] = test[\"ABSTRACT\"].apply(lambda x: remove_punctuations(x))\ntest[\"ABSTRACT\"] = test[\"ABSTRACT\"].apply(lambda x: clean_numbers(x))\ntest[\"ABSTRACT\"] = test[\"ABSTRACT\"].apply(lambda x: misspelled_words(x))","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Splitiing the data into train and validation (80:20) </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, val = train_test_split(df, test_size=0.2, random_state=0)\ntrain.shape, val.shape","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"((11203, 30), (2801, 30))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"<h3> Vectorizing train, validation and test dataset using Tfidf vectorizer</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidfvec = TfidfVectorizer(min_df=3, max_features=None, ngram_range=(1, 2), strip_accents='unicode', stop_words='english')\ntfidfvec.fit(df['ABSTRACT'])\ntrain_vec = tfidfvec.transform(train['ABSTRACT'])\nval_vec = tfidfvec.transform(val['ABSTRACT'])\ntest_vec = tfidfvec.transform(test['ABSTRACT'])\ntrain_vec.shape, val_vec.shape, test_vec.shape","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"((11203, 76621), (2801, 76621), (6002, 76621))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"<h5> Here after vectorizing we are stacking the remaining 4 features into csr format. Here if we use numpy array format instead of csr format then our RAM won't be able to suffice hence it is important to pass data to our model in csr format </h5>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = hstack((train_vec, train[['Computer Science', 'Mathematics', 'Physics', 'Statistics']]), format=\"csr\", dtype='float64')\nval_data = hstack((val_vec, val[['Computer Science', 'Mathematics', 'Physics', 'Statistics']]), format=\"csr\", dtype='float64')\ntest_data = hstack((test_vec, test[['Computer Science', 'Mathematics', 'Physics', 'Statistics']]), format=\"csr\", dtype='float64')\ntrain_data.shape, val_data.shape, test_data.shape","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"((11203, 76625), (2801, 76625), (6002, 76625))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"<h3> Using Grid search to find best hyperparameters </h3>\n<h5> Note: Since there was only single hyperparameter to tune hence i used GridSearchCV. If there are more hyperparameters it is wise to choose RandomizedSearchCV </h5>"},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {\n    'estimator__C': [10 ** x for x in range(-2, 3)]\n}\n\nestimator = OneVsRestClassifier(LogisticRegression(max_iter=500, n_jobs=-1))\nmodel = GridSearchCV(estimator, parameters, scoring='f1_micro', cv=5, n_jobs=-1, refit=False)\nmodel.fit(train_data, train[TARGET_COLS])\nbest_C = model.best_params_['estimator__C']\nprint('The best value of C is', best_C)","execution_count":14,"outputs":[{"output_type":"stream","text":"The best value of C is 100\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h3> Applying ML model using best hyperparameter and predicting on validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = OneVsRestClassifier(LogisticRegression(C = best_C, max_iter=500, n_jobs=-1))\nclf.fit(train_data, train[TARGET_COLS])\npred = clf.predict(val_data)\nf1_score(val[TARGET_COLS], pred, average='micro')","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"0.7315175097276265"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This is a simple hack which is used to find the optimal treshold to calculate the best F1 score\ndef get_best_thresholds(true, preds):\n    thresholds = [i/100 for i in range(100)]\n    best_thresholds = []\n    for idx in range(25):\n        f1_scores = [f1_score(true[:, idx], (preds[:, idx] > thresh) * 1) for thresh in thresholds]\n        best_thresh = thresholds[np.argmax(f1_scores)]\n        best_thresholds.append(best_thresh)\n    return best_thresholds","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = clf.predict_proba(val_data)\nbest_thresholds = get_best_thresholds(val[TARGET_COLS].values, val_preds)\nfor i, thresh in enumerate(best_thresholds):\n    val_preds[:, i] = (val_preds[:, i] > thresh) * 1\nf1_score(val[TARGET_COLS], val_preds, average='micro')","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"0.7864363403710812"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"<h4> As you can see above the F1 score after finding optimal tresholds has drastically improved from 0.73 to 0.78.\nSuch improvements can lead to gaining more rankings in competitions and hackathons</h4>"},{"metadata":{},"cell_type":"markdown","source":"<h3> Submitting the predictions </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"ss = pd.read_csv('../input/analyticsvidhya-hacklive3/SampleSubmission.csv')\npreds_test = clf.predict_proba(test_data)\n\nfor i, thresh in enumerate(best_thresholds):\n    preds_test[:, i] = (preds_test[:, i] > thresh) * 1\n\nss[TARGET_COLS] = preds_test\nss.to_csv('hacklive_submission', index = False)","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> PS: This is my first ever hackathon participation and kaggle notebook. Please provide feedbacks and upvote the notebook. </h4>"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}